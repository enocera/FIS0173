{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "NJET_BLHA='/Users/simon/packages/njet-3.1.1-1L/blha/'\n",
    "sys.path.append(NJET_BLHA)\n",
    "\n",
    "import numpy as np\n",
    "from pstools.rambo import generate, dot\n",
    "from njettools.njet_interface import *\n",
    "from nntools.model import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the number of training points (will later be split into NN train/test set)\n",
    "n_training_points = 10000\n",
    "n_points = 20000 # points for inference\n",
    "delta_cut = 0.01\n",
    "n_final = 4\n",
    "contract_file = 'NJ_contract_ee'+str(n_final)+'j_tree.lh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3348.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate 2 -> 3 phase-space points for training\n",
    "momenta, n_trials = generate(n_final, n_training_points, rts=1000., delta=delta_cut)\n",
    "momenta = momenta.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLP read in correctly\n"
     ]
    }
   ],
   "source": [
    "# start the NJet interface\n",
    "olp = njet.OLP()\n",
    "status = njet_init(contract_file)\n",
    "\n",
    "if status == True:\n",
    "    print (\"OLP read in correctly\")\n",
    "else:\n",
    "    print (\"seems to be a problem with the contract file...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mur = 100.\n",
    "alphas = 0.118\n",
    "alpha = 1/137."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NJ_treevals = [];\n",
    "for pt in range(n_training_points):\n",
    "    vals = olp.OLP_EvalSubProcess(1, momenta[pt], alphas=alphas, alpha=alpha, mur=mur, retlen=1)\n",
    "    NJ_treevals.append(vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFKSpartitions(mom):\n",
    "    nn = len(mom)\n",
    "    ss = [];\n",
    "    for i in range(2,nn-1):\n",
    "        for j in range(i+1,nn):\n",
    "            #print(i, j, i-2+(j-2)*(j-3)/2)\n",
    "            ss.append(2*dot(mom[i],mom[j]))\n",
    "    ss = np.array(ss)\n",
    "    DD = np.sum(1/ss)\n",
    "    SS = 1/ss/DD\n",
    "    return(SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02567362 0.09737629 0.01347357 0.23910074 0.07462899 0.54974679]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "parts = getFKSpartitions(momenta[0])\n",
    "n_FKSsectors = len(parts)\n",
    "\n",
    "print(parts)\n",
    "print(np.sum(parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NJ_treevals_FKS = []\n",
    "momenta_FKS = []\n",
    "for pt in range(n_training_points):\n",
    "    parts = getFKSpartitions(momenta[pt])\n",
    "    momenta_FKS.append(parts)\n",
    "    NJ_treevals_FKS.append(NJ_treevals[pt]*parts)\n",
    "\n",
    "momenta_FKS = np.array(momenta_FKS)\n",
    "NJ_treevals_FKS = np.array(NJ_treevals_FKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump generated data in case NJet and interface not available\n",
    "np.save(\"data/NJfks_ee\"+str(n_final)+\"j_tree_momenta_\"+str(delta_cut)+\".npy\", momenta)\n",
    "np.save(\"data/NJfks_ee\"+str(n_final)+\"j_tree_values_\"+str(delta_cut)+\".npy\", NJ_treevals)\n",
    "np.save(\"data/NJfks_ee\"+str(n_final)+\"j_tree_momenta_FKS_\"+str(delta_cut)+\".npy\", momenta_FKS)\n",
    "np.save(\"data/NJfks_ee\"+str(n_final)+\"j_tree_values_FKS_\"+str(delta_cut)+\".npy\", NJ_treevals_FKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start from here with pre-generated data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "momenta = np.load(\"data/NJfks_ee\"+str(n_final)+\"j_tree_momenta_\"+str(delta_cut)+\".npy\")\n",
    "NJ_treevals = np.load(\"data/NJfks_ee\"+str(n_final)+\"j_tree_values_\"+str(delta_cut)+\".npy\")\n",
    "momenta_FKS = np.load(\"data/NJfks_ee\"+str(n_final)+\"j_tree_momenta_FKS_\"+str(delta_cut)+\".npy\") \n",
    "NJ_treevals_FKS = np.load(\"data/NJfks_ee\"+str(n_final)+\"j_tree_values_FKS_\"+str(delta_cut)+\".npy\")\n",
    "n_training_points = len(momenta)\n",
    "n_FKSsectors = len(momenta_FKS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First train a basic or \"naive\" single model to test against #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Model(\n",
    "    (2+n_final)*4, # train with all momenta components \n",
    "    momenta[:n_training_points], # input data from Rambo PS generator\n",
    "    np.array(NJ_treevals) # data points from NJet evaluations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing training data using scaling =  standardise\n",
      "The training dataset has size (8000, 24)\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 99us/sample - loss: 1.1598 - val_loss: 1.2161\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 10us/sample - loss: 0.3703 - val_loss: 0.5986\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 10us/sample - loss: 0.2240 - val_loss: 0.5068\n"
     ]
    }
   ],
   "source": [
    "model, x_mean, x_std, y_mean, y_std = NN.fit(layers=[16,32,16], epoch_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now train each FKS partition separately #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing training data using scaling =  standardise\n",
      "The training dataset has size (8000, 24)\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 105us/sample - loss: 0.9444 - val_loss: 1.1340\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 10us/sample - loss: 0.3738 - val_loss: 0.5508\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 11us/sample - loss: 0.3114 - val_loss: 0.5315\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 11us/sample - loss: 0.2729 - val_loss: 0.5236\n",
      "processing training data using scaling =  standardise\n",
      "The training dataset has size (8000, 24)\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 118us/sample - loss: 1.3160 - val_loss: 0.8121\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 10us/sample - loss: 0.3157 - val_loss: 0.5084\n"
     ]
    }
   ],
   "source": [
    "NN_FKS = [Model(\n",
    "    (2+n_final)*4, # train with all momenta components \n",
    "    momenta[:n_training_points], # input data from Rambo PS generator\n",
    "    np.array(NJ_treevals_FKS[:,i]) # data points from NJet evaluations\n",
    ") for i in range(n_FKSsectors)]\n",
    "\n",
    "NNfitdata_FKS = [NN_FKS[i].fit(layers=[16,32,16], epoch_interval=100) for i in range(n_FKSsectors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test models against some new data points #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmomenta, n_trials2 = generate(n_final, n_points, rts=1000., delta=delta_cut)\n",
    "newmomenta = newmomenta.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NJ_treevals_test = [];\n",
    "for pt in range(n_points):\n",
    "    vals = olp.OLP_EvalSubProcess(1, newmomenta[pt], alphas=alphas, alpha=alpha, mur=mur, retlen=1)\n",
    "    NJ_treevals_test.append(vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values with single network\n",
    "x_standardized = NN.process_testing_data(moms=newmomenta,\n",
    "                                         x_mean=x_mean,x_std=x_std,y_mean=y_mean,y_std=y_std)\n",
    "mpred = model.predict(x_standardized)\n",
    "amp_pred = NN.destandardise_data(mpred.reshape(-1),\n",
    "                                 x_mean=x_mean,x_std=x_std,y_mean=y_mean,y_std=y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values with FKS networks\n",
    "amp_pred_FKS = []\n",
    "\n",
    "for i in range(n_FKSsectors):\n",
    "    model_tmp = NNfitdata_FKS[i][0]\n",
    "    x_m = NNfitdata_FKS[i][1]\n",
    "    x_s = NNfitdata_FKS[i][2]\n",
    "    y_m = NNfitdata_FKS[i][3]\n",
    "    y_s = NNfitdata_FKS[i][4]\n",
    "    \n",
    "    x_standardized = NN_FKS[i].process_testing_data(moms=newmomenta,\n",
    "                                         x_mean=x_m,x_std=x_s,y_mean=y_m,y_std=y_s)\n",
    "    mpred = model_tmp.predict(x_standardized)\n",
    "    amp_pred_FKS.append(NN_FKS[i].destandardise_data(mpred.reshape(-1),\n",
    "                                 x_mean=x_m,x_std=x_s,y_mean=y_m,y_std=y_s))\n",
    "    \n",
    "amp_pred_FKS = np.array(amp_pred_FKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_pred_FKS_sum = np.sum(amp_pred_FKS,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NJ_treevals_test[0:5])\n",
    "print(amp_pred[0:5])\n",
    "print(amp_pred_FKS_sum[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = getFKSpartitions(newmomenta[0])\n",
    "print(NJ_treevals_test[0]*part)\n",
    "print([amp_pred_FKS[i,0] for i in range(n_FKSsectors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (amp_pred-np.array(NJ_treevals_test))/(amp_pred+np.array(NJ_treevals_test))\n",
    "diff_FKS = (amp_pred_FKS_sum-np.array(NJ_treevals_test))/(amp_pred_FKS_sum+np.array(NJ_treevals_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybins = np.histogram_bin_edges(diff, bins=200, range=(-1.5,1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(diff, density=False, bins=mybins, label='single')\n",
    "plt.hist(diff_FKS, density=False, bins=mybins, label='FKS')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Data')\n",
    "plt.text(-1.5,4000,'delta = '+str(delta_cut))\n",
    "plt.text(-1.5,3700,'ee --> '+str(n_final)+'j')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdiff = np.log10(np.abs(diff))\n",
    "logdiff_FKS = np.log10(np.abs(diff_FKS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybins = np.histogram_bin_edges(logdiff, bins=200, range=(-4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(logdiff, density=False, bins=mybins, label='single')\n",
    "plt.hist(logdiff_FKS, density=False, bins=mybins, label='FKS')\n",
    "plt.ylabel('Log10 Accuracy')\n",
    "plt.xlabel('Data')\n",
    "plt.text(-4,450,'delta = '+str(delta_cut))\n",
    "plt.text(-4,400,'ee --> '+str(n_final)+'j')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
